## äº‘è®¡ç®—å’Œå¤§æ•°æ®åˆ†æ

è¿™èŠ‚è¯¾åŒ…å«ä»¥ä¸‹è¯é¢˜ï¼š

* Spark
* Hadoop
* Kubernetes
* Docker
* Flink
* MongoDB

è¯´èµ·äº‘è®¡ç®—ï¼Œä¼¼ä¹ç¦»ä¸å¼€å¾ˆå¤šçš„å·¥å…·ï¼ŒHadoopã€Hiveã€Hbaseã€ZooKeeperã€Dockerã€Kubernetesã€Sparkã€Kafkaã€MongoDBã€Flinkã€Druidã€Prestoã€Kylinã€Elastic Searchã€‚éƒ½æœ‰å¬è¿‡å—ã€‚è¿™äº›å·¥å…·æœ‰äº›æˆ‘æ˜¯ä»`å¤§æ•°æ®å·¥ç¨‹å¸ˆ`ã€`åˆ†å¸ƒå¼åç«¯å·¥ç¨‹å¸ˆ`ä¸Šæ‰¾åˆ°çš„ã€‚è¿™äº›éƒ½æ˜¯é«˜è–ªèŒä½ã€‚æˆ‘ä»¬è¯•ç€æŠŠä»–ä»¬éƒ½å®‰è£…ä¸Šï¼Œè¯•ç€æŠŠç©ä¸¤ä¸‹ã€‚


## åˆæ¢ Spark



å®˜ç½‘è¯´ï¼Œ`Spark`ç”¨æ¥å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„åˆ†æå¼•æ“ã€‚`spark`å°±æ˜¯ä¸€å¥—åº“ã€‚å®ƒä¼¼ä¹ä¸åƒ`Redis`é‚£æ ·åˆ†æˆæœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯ã€‚`spark`å°±æ˜¯åªåœ¨å®¢æˆ·ç«¯ä½¿ç”¨çš„ã€‚ä»å®˜ç½‘ä¸‹è½½äº†æœ€æ–°çš„ç‰ˆæœ¬ï¼Œ`spark-3.1.1-bin-hadoop3.2.tar`ã€‚

```shell
$ tree . -L 1
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ NOTICE
â”œâ”€â”€ R
â”œâ”€â”€ README.md
â”œâ”€â”€ RELEASE
â”œâ”€â”€ bin
â”œâ”€â”€ conf
â”œâ”€â”€ data
â”œâ”€â”€ examples
â”œâ”€â”€ jars
â”œâ”€â”€ kubernetes
â”œâ”€â”€ licenses
â”œâ”€â”€ python
â”œâ”€â”€ sbin
â””â”€â”€ yarn

11 directories, 4 files
```

ä¼¼ä¹å°±æ˜¯å„è¯­è¨€ç¼–å†™çš„ä¸€äº›åˆ†æåº“ã€‚



åŒæ—¶å®˜ç½‘è¯´å¯ä»¥åœ¨Pythonä¸Šç›´æ¥è£…ä¾èµ–åº“ã€‚`pip install pyspark`



```shell
$ pip install pyspark
Collecting pyspark
  Downloading pyspark-3.1.1.tar.gz (212.3 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212.3 MB 14 kB/s
Collecting py4j==0.10.9
  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198 kB 145 kB/s
Building wheels for collected packages: pyspark
  Building wheel for pyspark (setup.py) ... done
  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=0b8079e82f3a5bcadad99179902d8c8ff9f8eccad928a469c11b97abdc960b72
  Stored in directory: /Users/lzw/Library/Caches/pip/wheels/23/bf/e9/9f3500437422e2ab82246f25a51ee480a44d4efc6c27e50d33
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9 pyspark-3.1.1
```

è£…ä¸Šäº†ã€‚



è¿™ä¼šçœ‹å®˜ç½‘ï¼Œæœ‰äº›ä¾‹å­

```shell
./bin/run-example SparkPi 10
```

å“¦ï¼ŒåŸæ¥å¯ä»¥è¿è¡Œåˆšåˆšä¸‹è½½çš„å®‰è£…åŒ…é‡Œçš„ç¨‹åºã€‚ä½†å‡ºé”™äº†ã€‚

```shell
$ ./bin/run-example SparkPi 10
21/03/11 00:06:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/03/11 00:06:16 INFO ResourceUtils: No custom resources configured for spark.driver.
21/03/11 00:06:16 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
```



> Spark is a fast and general processing engine compatible with Hadoop data. It can run in Hadoop clusters through YARN or Spark's standalone mode, and it can process data in HDFS, HBase, Cassandra, Hive, and any Hadoop InputFormat. It is designed to perform both batch processing (similar to MapReduce) and new workloads like streaming, interactive queries, and machine learning.



å‡ºç°äº†å¥½å‡ æ¬¡`hadoop`ã€‚è°·æ­Œäº†`spark depends hadoop `ä¹‹åï¼Œæ‰¾åˆ°è¿™æ ·ä¸€æ®µè¯ã€‚çœ‹æ¥è¿™ä¾èµ–äº`Hadoop`æ ¼å¼çš„æ•°æ®ã€‚è®©æˆ‘ä»¬å…ˆç ”ç©¶ `Hadoop`ã€‚



## Hadoop



ç®€å•çœ‹äº†å®˜ç½‘åã€‚æ¥å®‰è£…ä¸€ä¸‹ã€‚

```shell
brew install hadoop
```

å®‰è£…çš„è¿‡ç¨‹ä¸­ï¼Œæ¥äº†è§£ä¸€ä¸‹ã€‚



> The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.



å°±æ˜¯è¯´ Hadoop æ˜¯ä¸€å¥—æ¡†æ¶ï¼Œæ¥å¤„ç†åˆ†å¸ƒå¼çš„æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†å¯èƒ½åˆ†éƒ¨åœ¨å¾ˆå¤šè®¡ç®—æœºä¸Šã€‚ç”¨å¾ˆç®€å•çš„ç¼–ç¨‹æ¨¡å‹æ¥å¤„ç†ã€‚å®ƒæ˜¯è®¾è®¡æ¥ä»å•ä¸€æœåŠ¡å™¨æ‰©å±•åˆ°åƒå°æœºå™¨çš„ã€‚ä¸å…¶ä¾èµ–äºç¡¬ä»¶çš„é«˜å¯ç”¨ï¼Œè¿™ä¸ªåº“åˆ™è®¾è®¡æ¥åœ¨åº”ç”¨å±‚å°±èƒ½æ£€æŸ¥å’Œå¤„ç†é”™è¯¯ã€‚å› æ­¤èƒ½å°†é«˜å¯ç”¨çš„æœåŠ¡éƒ¨ç½²åˆ°é›†ç¾¤ä¸­ï¼Œè™½ç„¶é›†ç¾¤ä¸­çš„æ¯å°ç”µè„‘éƒ½å¯èƒ½å¯¼è‡´å¤±è´¥ã€‚



```shell
$ brew install hadoop
Error:
  homebrew-core is a shallow clone.
  homebrew-cask is a shallow clone.
To `brew update`, first run:
  git -C /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core fetch --unshallow
  git -C /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask fetch --unshallow
These commands may take a few minutes to run due to the large size of the repositories.
This restriction has been made on GitHub's request because updating shallow
clones is an extremely expensive operation due to the tree layout and traffic of
Homebrew/homebrew-core and Homebrew/homebrew-cask. We don't do this for you
automatically to avoid repeatedly performing an expensive unshallow operation in
CI systems (which should instead be fixed to not use shallow clones). Sorry for
the inconvenience!
==> Downloading https://homebrew.bintray.com/bottles/openjdk-15.0.1.big_sur.bottle.tar.gz
Already downloaded: /Users/lzw/Library/Caches/Homebrew/downloads/d1e3ece4af1d225bc2607eaa4ce85a873d2c6d43757ae4415d195751bc431962--openjdk-15.0.1.big_sur.bottle.tar.gz
==> Downloading https://www.apache.org/dyn/closer.lua?path=hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz
Already downloaded: /Users/lzw/Library/Caches/Homebrew/downloads/764c6a0ea7352bb8bb505989feee1b36dc628c2dcd6b93fef1ca829d191b4e1e--hadoop-3.3.0.tar.gz
==> Installing dependencies for hadoop: openjdk
==> Installing hadoop dependency: openjdk
==> Pouring openjdk-15.0.1.big_sur.bottle.tar.gz
==> Caveats
For the system Java wrappers to find this JDK, symlink it with
  sudo ln -sfn /usr/local/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk

openjdk is keg-only, which means it was not symlinked into /usr/local,
because it shadows the macOS `java` wrapper.

If you need to have openjdk first in your PATH run:
  echo 'export PATH="/usr/local/opt/openjdk/bin:$PATH"' >> /Users/lzw/.bash_profile

For compilers to find openjdk you may need to set:
  export CPPFLAGS="-I/usr/local/opt/openjdk/include"

==> Summary
ğŸº  /usr/local/Cellar/openjdk/15.0.1: 614 files, 324.9MB
==> Installing hadoop
ğŸº  /usr/local/Cellar/hadoop/3.3.0: 21,819 files, 954.7MB, built in 2 minutes 15 seconds
==> Upgrading 1 dependent:
maven 3.3.3 -> 3.6.3_1
==> Upgrading maven 3.3.3 -> 3.6.3_1
==> Downloading https://www.apache.org/dyn/closer.lua?path=maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
==> Downloading from https://mirror.olnevhost.net/pub/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
######################################################################## 100.0%
Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink bin/mvn
Target /usr/local/bin/mvn
is a symlink belonging to maven. You can unlink it:
  brew unlink maven

To force the link and overwrite all conflicting files:
  brew link --overwrite maven

To list all files that would be deleted:
  brew link --overwrite --dry-run maven

Possible conflicting files are:
/usr/local/bin/mvn -> /usr/local/Cellar/maven/3.3.3/bin/mvn
/usr/local/bin/mvnDebug -> /usr/local/Cellar/maven/3.3.3/bin/mvnDebug
/usr/local/bin/mvnyjp -> /usr/local/Cellar/maven/3.3.3/bin/mvnyjp
==> Summary
ğŸº  /usr/local/Cellar/maven/3.6.3_1: 87 files, 10.7MB, built in 7 seconds
Removing: /usr/local/Cellar/maven/3.3.3... (92 files, 9MB)
==> Checking for dependents of upgraded formulae...
==> No broken dependents found!
==> Caveats
==> openjdk
For the system Java wrappers to find this JDK, symlink it with
  sudo ln -sfn /usr/local/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk

openjdk is keg-only, which means it was not symlinked into /usr/local,
because it shadows the macOS `java` wrapper.

If you need to have openjdk first in your PATH run:
  echo 'export PATH="/usr/local/opt/openjdk/bin:$PATH"' >> /Users/lzw/.bash_profile

For compilers to find openjdk you may need to set:
  export CPPFLAGS="-I/usr/local/opt/openjdk/include"
```

æ³¨æ„åˆ°`brew`çš„è¾“å‡ºæ—¥å¿—ä¸­`maven`æ²¡æœ‰å¾ˆå¥½åœ°è¢«é“¾æ¥ã€‚æ¥ä¸‹æ¥ï¼Œè¿›è¡Œå¼ºåˆ¶é“¾æ¥åˆ°`3.6.3_1`ç‰ˆæœ¬ã€‚

```shell
  brew link --overwrite maven
```

`Hadoop`å°±å®‰è£…æˆåŠŸäº†ã€‚



> ## Modules
>
> The project includes these modules:
>
> - **Hadoop Common**: The common utilities that support the other Hadoop modules.
> - **Hadoop Distributed File System (HDFSâ„¢)**: A distributed file system that provides high-throughput access to application data.
> - **Hadoop YARN**: A framework for job scheduling and cluster resource management.
> - **Hadoop MapReduce**: A YARN-based system for parallel processing of large data sets.
> - **Hadoop Ozone**: An object store for Hadoop.



è¯´æœ‰è¿™äº›æ¨¡å—ã€‚è¿™ä¼šæ•²å…¥`hadoop`å‡ºç°äº†ï¼š

```shell
$ hadoop
Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
 or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]
  where CLASSNAME is a user-provided Java class

  OPTIONS is none or any of:

--config dir                     Hadoop config directory
--debug                          turn on shell script debug mode
--help                           usage information
buildpaths                       attempt to add class files from build tree
hostnames list[,of,host,names]   hosts to use in slave mode
hosts filename                   list of hosts to use in slave mode
loglevel level                   set the log4j level for this command
workers                          turn on worker mode

  SUBCOMMAND is one of:


    Admin Commands:

daemonlog     get/set the log level for each daemon

    Client Commands:

archive       create a Hadoop archive
checknative   check native Hadoop and compression libraries availability
classpath     prints the class path needed to get the Hadoop jar and the required libraries
conftest      validate configuration XML files
credential    interact with credential providers
distch        distributed metadata changer
distcp        copy file or directories recursively
dtutil        operations related to delegation tokens
envvars       display computed Hadoop environment variables
fs            run a generic filesystem user client
gridmix       submit a mix of synthetic job, modeling a profiled from production load
jar <jar>     run a jar file. NOTE: please use "yarn jar" to launch YARN applications, not this command.
jnipath       prints the java.library.path
kdiag         Diagnose Kerberos Problems
kerbname      show auth_to_local principal conversion
key           manage keys via the KeyProvider
rumenfolder   scale a rumen input trace
rumentrace    convert logs into a rumen trace
s3guard       manage metadata on S3
trace         view and modify Hadoop tracing settings
version       print the version

    Daemon Commands:

kms           run KMS, the Key Management Server
registrydns   run the registry DNS server

SUBCOMMAND may print help when invoked w/o parameters or with -h.
```

å®˜ç½‘ç»™äº†äº›ä¾‹å­ã€‚



```shell
  $ mkdir input
  $ cp etc/hadoop/*.xml input
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output 'dfs[a-z.]+'
  $ cat output/*
```



æ³¨æ„åˆ°æœ‰`share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar`ã€‚è¿™æ„å‘³ç€ä¹Ÿè®¸æœ‰äº›æ ·ä¾‹æ–‡ä»¶æˆ‘ä»¬æ²¡æœ‰å¾—åˆ°ã€‚çŒœæµ‹ç”¨`Homebrew`å®‰è£…ä¼šæ²¡æœ‰è¿™äº›æ–‡ä»¶ã€‚æˆ‘ä»¬ä»å®˜ç½‘ä¸‹è½½äº†å®‰è£…æ–‡ä»¶åŒ…ã€‚

```shell
$ tree . -L 1
.
â”œâ”€â”€ LICENSE-binary
â”œâ”€â”€ LICENSE.txt
â”œâ”€â”€ NOTICE-binary
â”œâ”€â”€ NOTICE.txt
â”œâ”€â”€ README.txt
â”œâ”€â”€ bin
â”œâ”€â”€ etc
â”œâ”€â”€ include
â”œâ”€â”€ lib
â”œâ”€â”€ libexec
â”œâ”€â”€ licenses-binary
â”œâ”€â”€ sbin
â””â”€â”€ share
```

å‡ºç°äº†`share`ç›®å½•ã€‚ç„¶è€Œ`Homebrew`çœŸçš„æ²¡æœ‰é™„åŠ çš„è¿™äº›æ–‡ä»¶å—ã€‚æ‰¾åˆ°`Homebrew`å®‰è£…çš„ç›®å½•ã€‚

```shell
$ type hadoop
hadoop is /usr/local/bin/hadoop
$ ls -alrt /usr/local/bin/hadoop
lrwxr-xr-x  1 lzw  admin  33 Mar 11 00:48 /usr/local/bin/hadoop -> ../Cellar/hadoop/3.3.0/bin/hadoop
$ cd /usr/local/Cellar/hadoop/3.3.0
```



è¿™æ˜¯åœ¨`/usr/local/Cellar/hadoop/3.3.0/libexec/share/hadoop`ä¸‹æ‰“å°çš„ç›®å½•æ ‘

```shell
$ tree . -L 2
.
â”œâ”€â”€ client
â”‚Â Â  â”œâ”€â”€ hadoop-client-api-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-client-minicluster-3.3.0.jar
â”‚Â Â  â””â”€â”€ hadoop-client-runtime-3.3.0.jar
â”œâ”€â”€ common
â”‚Â Â  â”œâ”€â”€ hadoop-common-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-common-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-kms-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-nfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-registry-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ jdiff
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ sources
â”‚Â Â  â””â”€â”€ webapps
â”œâ”€â”€ hdfs
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-client-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-client-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-httpfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-native-client-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-native-client-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-nfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-rbf-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-rbf-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ jdiff
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ sources
â”‚Â Â  â””â”€â”€ webapps
â”œâ”€â”€ mapreduce
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-app-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-common-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-core-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-hs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-hs-plugins-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-jobclient-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-jobclient-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-nativetask-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-shuffle-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-uploader-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-examples-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ jdiff
â”‚Â Â  â”œâ”€â”€ lib-examples
â”‚Â Â  â””â”€â”€ sources
â”œâ”€â”€ tools
â”‚Â Â  â”œâ”€â”€ dynamometer
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ resourceestimator
â”‚Â Â  â”œâ”€â”€ sls
â”‚Â Â  â””â”€â”€ sources
â””â”€â”€ yarn
    â”œâ”€â”€ csi
    â”œâ”€â”€ hadoop-yarn-api-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-applications-catalog-webapp-3.3.0.war
    â”œâ”€â”€ hadoop-yarn-applications-distributedshell-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-applications-mawo-core-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-applications-unmanaged-am-launcher-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-client-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-common-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-registry-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-applicationhistoryservice-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-common-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-nodemanager-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-resourcemanager-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-router-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-sharedcachemanager-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-tests-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-timeline-pluginstorage-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-server-web-proxy-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-services-api-3.3.0.jar
    â”œâ”€â”€ hadoop-yarn-services-core-3.3.0.jar
    â”œâ”€â”€ lib
    â”œâ”€â”€ sources
    â”œâ”€â”€ test
    â”œâ”€â”€ timelineservice
    â”œâ”€â”€ webapps
    â””â”€â”€ yarn-service-examples
```

å¯ä»¥çœ‹åˆ°æœ‰å¾ˆå¤šçš„`jar`åŒ…ã€‚

```shell
$ mkdir input
$ ls
bin			hadoop-config.sh	hdfs-config.sh		libexec			sbin			yarn-config.sh
etc			hadoop-functions.sh	input			mapred-config.sh	share
$ cp etc/hadoop/*.xml input
$ cd input/
$ ls
capacity-scheduler.xml	hadoop-policy.xml	hdfs-site.xml		kms-acls.xml		mapred-site.xml
core-site.xml		hdfs-rbf-site.xml	httpfs-site.xml		kms-site.xml		yarn-site.xml
$ cd ..
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output 'dfs[a-z.]+'
JAR does not exist or is not a normal file: /usr/local/Cellar/hadoop/3.3.0/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar
$
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep input output 'dfs[a-z.]+'
2021-03-11 01:54:30,791 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2021-03-11 01:54:31,115 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2021-03-11 01:54:31,232 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
...
```

ç…§ç€å®˜ç½‘çš„ä¾‹å­æ•²ã€‚æ³¨æ„åˆ°`bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input `ï¼Œè¿™é‡Œæ˜¯çš„`jar`åŒ…å‰æœ‰ç‰ˆæœ¬å·ã€‚å› æ­¤è¦æ¢æˆæˆ‘ä»¬çš„`3.3.0`ã€‚



æ—¥å¿—çš„æœ€åï¼š

```shell
2021-03-11 01:54:35,374 INFO mapreduce.Job:  map 100% reduce 100%
2021-03-11 01:54:35,374 INFO mapreduce.Job: Job job_local2087514596_0002 completed successfully
2021-03-11 01:54:35,377 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=1204316
		FILE: Number of bytes written=3565480
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Map output bytes=17
		Map output materialized bytes=25
		Input split bytes=141
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=25
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=57
		Total committed heap usage (bytes)=772800512
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=123
	File Output Format Counters
		Bytes Written=23
```

ç»§ç»­çœ‹çœ‹ã€‚

```shell
$ cat output/*
1	dfsadmin
```



è¿™åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ã€‚ä¸è¦ç´§ï¼Œæ€»ä¹‹æˆ‘ä»¬æŠŠ`Hadoop`è·‘èµ·æ¥äº†ã€‚å¹¶ä¸”è¿è¡Œäº†ç¬¬ä¸€ä¸ªå•æœºç‰ˆçš„è®¡ç®—ä¾‹å­ã€‚



## Spark

å›åˆ° Spark ä¸Šã€‚çœ‹ä¸€ä¸ªä¾‹å­ã€‚

```python
text_file = sc.textFile("hdfs://...")
counts = text_file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("hdfs://...")
```

è¿™é‡Œå‡ºç°äº†`hdfs`æ–‡ä»¶ã€‚æŸ¥é˜…åï¼Œå¾—çŸ¥å¯ä»¥è¿™æ ·åˆ›å»º`hdfs`æ–‡ä»¶ï¼š

```shell
hdfs dfs -mkdir /test
```



æ¥çœ‹çœ‹`hdfs`å‘½ä»¤ã€‚

```shell
$ hdfs
Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]

  OPTIONS is none or any of:

--buildpaths                       attempt to add class files from build tree
--config dir                       Hadoop config directory
--daemon (start|status|stop)       operate on a daemon
--debug                            turn on shell script debug mode
--help                             usage information
--hostnames list[,of,host,names]   hosts to use in worker mode
--hosts filename                   list of hosts to use in worker mode
--loglevel level                   set the log4j level for this command
--workers                          turn on worker mode

  SUBCOMMAND is one of:


    Admin Commands:

cacheadmin           configure the HDFS cache
crypto               configure HDFS encryption zones
debug                run a Debug Admin to execute HDFS debug commands
dfsadmin             run a DFS admin client
dfsrouteradmin       manage Router-based federation
ec                   run a HDFS ErasureCoding CLI
fsck                 run a DFS filesystem checking utility
haadmin              run a DFS HA admin client
jmxget               get JMX exported values from NameNode or DataNode.
oev                  apply the offline edits viewer to an edits file
oiv                  apply the offline fsimage viewer to an fsimage
oiv_legacy           apply the offline fsimage viewer to a legacy fsimage
storagepolicies      list/get/set/satisfyStoragePolicy block storage policies

    Client Commands:

classpath            prints the class path needed to get the hadoop jar and the required libraries
dfs                  run a filesystem command on the file system
envvars              display computed Hadoop environment variables
fetchdt              fetch a delegation token from the NameNode
getconf              get config values from configuration
groups               get the groups which users belong to
lsSnapshottableDir   list all snapshottable dirs owned by the current user
snapshotDiff         diff two snapshots of a directory or diff the current directory contents with a snapshot
version              print the version

    Daemon Commands:

balancer             run a cluster balancing utility
datanode             run a DFS datanode
dfsrouter            run the DFS router
diskbalancer         Distributes data evenly among disks on a given node
httpfs               run HttpFS server, the HDFS HTTP Gateway
journalnode          run the DFS journalnode
mover                run a utility to move block replicas across storage types
namenode             run the DFS namenode
nfs3                 run an NFS version 3 gateway
portmap              run a portmap service
secondarynamenode    run the DFS secondary namenode
sps                  run external storagepolicysatisfier
zkfc                 run the ZK Failover Controller daemon

SUBCOMMAND may print help when invoked w/o parameters or with -h.
```



ç»§ç»­ä¿®æ”¹ä»£ç ã€‚



```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]")\
           .config('spark.driver.bindAddress', '127.0.0.1')\
           .getOrCreate()
sc = spark.sparkContext

text_file = sc.textFile("a.txt")
counts = text_file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("b.txt")
```



æ³¨æ„åˆ°`.config('spark.driver.bindAddress', '127.0.0.1')`å¾ˆé‡è¦ã€‚å¦åˆ™ä¼šæŠ¥é”™è¯¯`Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address`ã€‚



ç„¶è€Œï¼Œè¿™æ—¶åˆå‡ºç°äº†é”™è¯¯ã€‚

```shell
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
```

è¡¨ç¤ºè¿è¡Œäº†ä¸åŒç‰ˆæœ¬çš„`Python`ã€‚



ä¿®æ”¹`.bash_profile`ï¼š

```shell
PYSPARK_PYTHON=/usr/local/Cellar/python@3.9/3.9.1_6/bin/python3
PYSPARK_DRIVER_PYTHON=/usr/local/Cellar/python@3.9/3.9.1_6/bin/python3
```

ç„¶è€Œè¿˜æ˜¯æŠ¥åŒæ ·çš„é”™ã€‚äº†è§£ä¸€ç•ªåï¼Œå¯èƒ½æ˜¯å› ä¸º`spark`è¿è¡Œçš„æ—¶å€™ï¼Œæ²¡æœ‰è½½å…¥è¿™ä¸ªç¯å¢ƒå˜é‡ï¼Œæ²¡æœ‰ä½¿ç”¨ç»ˆç«¯é»˜è®¤çš„ç¯å¢ƒå˜é‡ã€‚



éœ€è¦åœ¨ä»£ç é‡Œè®¾ç½®ï¼š

```python
import os

# Set spark environments
os.environ['PYSPARK_PYTHON'] = '/usr/local/Cellar/python@3.9/3.9.1_6/bin/python3'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/Cellar/python@3.9/3.9.1_6/bin/python3'
```

è¿™ä¼šè¿è¡Œã€‚



```shell
$ python sc.py
21/03/11 02:54:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
PythonRDD[6] at RDD at PythonRDD.scala:53
```

è¿™æ—¶ç”Ÿæˆäº†`b.txt`ã€‚

```shell
â”œâ”€â”€ b.txt
â”‚Â Â  â”œâ”€â”€ _SUCCESS
â”‚Â Â  â”œâ”€â”€ part-00000
â”‚Â Â  â””â”€â”€ part-00001
```

æ‰“å¼€ä¸€ä¸‹ã€‚

```shell
$ cat b.txt/part-00000
('college', 1)
('two', 1)
('things', 2)
('worked', 1)
('on,', 1)
('of', 8)
('school,', 2)
('writing', 2)
('programming.', 1)
("didn't", 4)
('then,', 1)
('probably', 1)
('are:', 1)
('short', 1)
('awful.', 1)
('They', 1)
('plot,', 1)
('just', 1)
('characters', 1)
('them', 2)
...
```

æˆåŠŸäº†ï¼è¿™æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ã€‚è¿™å°±åƒåœ¨`Hadoop`ä¾‹å­é‡Œçš„ã€‚

```shell
$ cat output/*
1	dfsadmin
```

è¿™äº›æ–‡ä»¶å°±å«`HDFS`ã€‚å¯è§è¿™é‡Œç”¨`Spark`æ¥ç»Ÿè®¡å•è¯ã€‚çŸ­çŸ­å‡ å¥ï¼Œå¾ˆæ–¹ä¾¿çš„æ ·å­ã€‚



## Kubernetes



æ¥ä¸‹æ¥æ£é¼“ä¸€ä¸‹`Kubernetes`ï¼Œä¹Ÿå«`k8s`ï¼Œä¸­é—´çš„8ä¸ªå­—æ¯ç®€å†™ä¸º8ã€‚å®ƒæ˜¯ä¸€å¥—å¼€æºç³»ç»Ÿï¼Œæ¥è‡ªåŠ¨åŒ–éƒ¨ç½²ã€æ‰©å¢å’Œç®¡ç†å®¹å™¨ç¨‹åºçš„ã€‚



`kubectl`å‘½ä»¤è¡Œå·¥å…·æ˜¯ç”¨æ¥è¿è¡Œä¸€äº›å‘½ä»¤æ“ä½œk8sé›†ç¾¤ã€‚å¯ä»¥ç”¨å®ƒæ¥éƒ¨ç½²åº”ç”¨ã€æŸ¥çœ‹å’Œç®¡ç†é›†ç¾¤èµ„æºï¼Œæ¥æŸ¥çœ‹æ—¥å¿—ã€‚



åŒæ ·å¯ä»¥ç”¨Homebrewæ¥å®‰è£…ã€‚

```shell
brew install kubectl 
```



è¾“å‡ºæ—¥å¿—ï¼š

```shell
==> Downloading https://homebrew.bintray.com/bottles/kubernetes-cli-1.20.1.big_sur.bottle.tar.gz
==> Downloading from https://d29vzk4ow07wi7.cloudfront.net/0b4f08bd1d47cb913d7cd4571e3394c6747dfbad7ff114c5589c8396c1085ecf?response-content-disposition=a
######################################################################## 100.0%
==> Pouring kubernetes-cli-1.20.1.big_sur.bottle.tar.gz
==> Caveats
Bash completion has been installed to:
  /usr/local/etc/bash_completion.d
==> Summary
ğŸº  /usr/local/Cellar/kubernetes-cli/1.20.1: 246 files, 46.1MB
```

è£…å¥½äº†ã€‚



```shell
$ kubectl version --client
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.1", GitCommit:"c4d752765b3bbac2237bf87cf0b1c2e307844666", GitTreeState:"clean", BuildDate:"2020-12-19T08:38:20Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"darwin/amd64"}
```



```shell
$ kubectl
kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/

Basic Commands (Beginner):
  create        Create a resource from a file or from stdin.
  expose        Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service
  run           Run a particular image on the cluster
  set           Set specific features on objects

Basic Commands (Intermediate):
  explain       Documentation of resources
  get           Display one or many resources
  edit          Edit a resource on the server
  delete        Delete resources by filenames, stdin, resources and names, or by resources and label selector

Deploy Commands:
  rollout       Manage the rollout of a resource
  scale         Set a new size for a Deployment, ReplicaSet or Replication Controller
  autoscale     Auto-scale a Deployment, ReplicaSet, or ReplicationController

Cluster Management Commands:
  certificate   Modify certificate resources.
  cluster-info  Display cluster info
  top           Display Resource (CPU/Memory/Storage) usage.
  cordon        Mark node as unschedulable
  uncordon      Mark node as schedulable
  drain         Drain node in preparation for maintenance
  taint         Update the taints on one or more nodes

Troubleshooting and Debugging Commands:
  describe      Show details of a specific resource or group of resources
  logs          Print the logs for a container in a pod
  attach        Attach to a running container
  exec          Execute a command in a container
  port-forward  Forward one or more local ports to a pod
  proxy         Run a proxy to the Kubernetes API server
  cp            Copy files and directories to and from containers.
  auth          Inspect authorization
  debug         Create debugging sessions for troubleshooting workloads and nodes

Advanced Commands:
  diff          Diff live version against would-be applied version
  apply         Apply a configuration to a resource by filename or stdin
  patch         Update field(s) of a resource
  replace       Replace a resource by filename or stdin
  wait          Experimental: Wait for a specific condition on one or many resources.
  kustomize     Build a kustomization target from a directory or a remote url.

Settings Commands:
  label         Update the labels on a resource
  annotate      Update the annotations on a resource
  completion    Output shell completion code for the specified shell (bash or zsh)

Other Commands:
  api-resources Print the supported API resources on the server
  api-versions  Print the supported API versions on the server, in the form of "group/version"
  config        Modify kubeconfig files
  plugin        Provides utilities for interacting with plugins.
  version       Print the client and server version information

Usage:
  kubectl [flags] [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
```



æ¥åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  minReadySeconds: 5
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

```



```shell
$ kubectl apply -f simple_deployment.yaml
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```



```shell
$ kubectl cluster-info

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```



å½“ç”¨å®˜ç½‘çš„ç»ˆç«¯è¯•ç€è¿è¡Œä¸‹ã€‚

```shell
$ start.sh
Starting Kubernetes...minikube version: v1.8.1
commit: cbda04cf6bbe65e987ae52bb393c10099ab62014
* minikube v1.8.1 on Ubuntu 18.04
* Using the none driver based on user configuration
* Running on localhost (CPUs=2, Memory=2460MB, Disk=145651MB) ...
* OS release is Ubuntu 18.04.4 LTS

* Preparing Kubernetes v1.17.3 on Docker 19.03.6 ...
  - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf
* Launching Kubernetes ... 
* Enabling addons: default-storageclass, storage-provisioner
* Configuring local host environment ...
* Done! kubectl is now configured to use "minikube"
* The 'dashboard' addon is enabled
Kubernetes Started
```



ç»§ç»­å›åˆ°æˆ‘ä»¬çš„ç»ˆç«¯ã€‚

```shell
$ kubectl version --client
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.1", GitCommit:"c4d752765b3bbac2237bf87cf0b1c2e307844666", GitTreeState:"clean", BuildDate:"2020-12-19T08:38:20Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"darwin/amd64"}
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.1", GitCommit:"c4d752765b3bbac2237bf87cf0b1c2e307844666", GitTreeState:"clean", BuildDate:"2020-12-19T08:38:20Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"darwin/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

æœ‰æ„æ€çš„æ˜¯åŠ ä¸Š`--client` é€‰é¡¹å¹¶æ²¡æœ‰æŠ¥é”™ã€‚



æ–‡æ¡£è¯´ï¼Œéœ€è¦å…ˆå®‰è£…`Minikube`ã€‚

```shell
$ brew install minikube
==> Downloading https://homebrew.bintray.com/bottles/minikube-1.16.0.big_sur.bottle.tar.gz
==> Downloading from https://d29vzk4ow07wi7.cloudfront.net/1b6d7d1b97b11b6b07e4fa531c2dc21770da290da9b2816f360fd923e00c85fc?response-content-disposition=a
######################################################################## 100.0%
==> Pouring minikube-1.16.0.big_sur.bottle.tar.gz
==> Caveats
Bash completion has been installed to:
  /usr/local/etc/bash_completion.d
==> Summary
ğŸº  /usr/local/Cellar/minikube/1.16.0: 8 files, 64.6MB
```



```shell
$ minikube start
ğŸ˜„  minikube v1.16.0 on Darwin 11.2.2
ğŸ‰  minikube 1.18.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.18.1
ğŸ’¡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

âœ¨  Automatically selected the virtualbox driver
ğŸ’¿  Downloading VM boot image ...
    > minikube-v1.16.0.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s
    > minikube-v1.16.0.iso: 212.62 MiB / 212.62 MiB [] 100.00% 5.32 MiB p/s 40s
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸ’¾  Downloading Kubernetes v1.20.0 preload ...
    > preloaded-images-k8s-v8-v1....: 491.00 MiB / 491.00 MiB  100.00% 7.52 MiB
ğŸ”¥  Creating virtualbox VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
â—  This VM is having trouble accessing https://k8s.gcr.io
ğŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
ğŸ³  Preparing Kubernetes v1.20.0 on Docker 20.10.0 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”  Verifying Kubernetes components...
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```



æ¥ç€æ¥è®¿é—®è¿™ä¸ªé›†ç¾¤ã€‚

```shell
$ kubectl get po -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-74ff55c5b-ndbcr            1/1     Running   0          60s
kube-system   etcd-minikube                      0/1     Running   0          74s
kube-system   kube-apiserver-minikube            1/1     Running   0          74s
kube-system   kube-controller-manager-minikube   1/1     Running   0          74s
kube-system   kube-proxy-g2296                   1/1     Running   0          60s
kube-system   kube-scheduler-minikube            0/1     Running   0          74s
kube-system   storage-provisioner                1/1     Running   1          74s
```



æ¥æ‰“å¼€`minikube`çš„æ§åˆ¶æ¿ã€‚

```shell
$ minikube dashboard
ğŸ”Œ  Enabling dashboard ...
ğŸ¤”  Verifying dashboard health ...
ğŸš€  Launching proxy ...
ğŸ¤”  Verifying proxy health ...
ğŸ‰  Opening http://127.0.0.1:50030/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...
```



![k8s](./img/k8s.png)



å¦‚ä½•å…³æ‰å‘¢ã€‚



```shell
$ minikube
minikube provisions and manages local Kubernetes clusters optimized for development workflows.

Basic Commands:
  start          Starts a local Kubernetes cluster
  status         Gets the status of a local Kubernetes cluster
  stop           Stops a running local Kubernetes cluster
  delete         Deletes a local Kubernetes cluster
  dashboard      Access the Kubernetes dashboard running within the minikube cluster
  pause          pause Kubernetes
  unpause        unpause Kubernetes

Images Commands:
  docker-env     Configure environment to use minikube's Docker daemon
  podman-env     Configure environment to use minikube's Podman service
  cache          Add, delete, or push a local image into minikube

Configuration and Management Commands:
  addons         Enable or disable a minikube addon
  config         Modify persistent configuration values
  profile        Get or list the current profiles (clusters)
  update-context Update kubeconfig in case of an IP or port change

Networking and Connectivity Commands:
  service        Returns a URL to connect to a service
  tunnel         Connect to LoadBalancer services

Advanced Commands:
  mount          Mounts the specified directory into minikube
  ssh            Log into the minikube environment (for debugging)
  kubectl        Run a kubectl binary matching the cluster version
  node           Add, remove, or list additional nodes

Troubleshooting Commands:
  ssh-key        Retrieve the ssh identity key path of the specified node
  ssh-host       Retrieve the ssh host key of the specified node
  ip             Retrieves the IP address of the specified node
  logs           Returns logs to debug a local Kubernetes cluster
  update-check   Print current and latest version number
  version        Print the version of minikube

Other Commands:
  completion     Generate command completion for a shell

Use "minikube <command> --help" for more information about a given command.
```



å¯è§æ˜¯`minikube stop`ã€‚



å›åˆ°`kuberntes`ï¼Œç°åœ¨å·¥ä½œæ­£å¸¸äº†ã€‚



```shell
$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.99.100:8443
KubeDNS is running at https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```



å½“æˆ‘ä»¬æ‰“å¼€`https://192.168.99.100:8443`æ—¶ï¼Œæµè§ˆå™¨æ˜¾ç¤ºï¼š



```json
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {
    
  },
  "code": 403
}
```



è®¿é—®`https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy`ï¼š



```json
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "services \"kube-dns:dns\" is forbidden: User \"system:anonymous\" cannot get resource \"services/proxy\" in API group \"\" in the namespace \"kube-system\"",
  "reason": "Forbidden",
  "details": {
    "name": "kube-dns:dns",
    "kind": "services"
  },
  "code": 403
}
```



æ¥è¯•è¯•åˆšåˆšé‚£ä¸ªé…ç½®ã€‚

```shell
$ kubectl apply -f simple_deployment.yaml
deployment.apps/nginx-deployment created
```



æœ‰ç‚¹é—®é¢˜ã€‚ç„¶è€Œåˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»æŠŠ`kubernetes`è·‘èµ·æ¥äº†ã€‚å…ˆç»“æŸæ‰ã€‚åç»­å†ç©ã€‚



```shell
$ minikube stop
âœ‹  Stopping node "minikube"  ...
ğŸ›‘  1 nodes stopped.
```



æ£€æŸ¥æ˜¯å¦ç»“æŸã€‚

```shell
w$ minikube dashboard
ğŸ¤·  The control plane node must be running for this command
ğŸ‘‰  To start a cluster, run: "minikube start"
```





## Docker



`Docker`ä¹Ÿæ˜¯ä¸€ç§å®¹å™¨å¹³å°ï¼Œæ¥å¸®åŠ©åŠ é€Ÿåˆ›å»ºã€åˆ†äº«ã€è¿è¡Œç°ä»£åº”ç”¨ã€‚ä»å®˜ç½‘ä¸‹è½½åº”ç”¨ã€‚

![docker](./img/docker.png)

ç”¨å®¢æˆ·ç«¯æœ‰ç‚¹å¡ã€‚è®©æˆ‘ä»¬ç”¨å‘½ä»¤è¡Œã€‚

```docker
$ docker

Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Options:
      --config string      Location of client config files (default "/Users/lzw/.docker")
  -c, --context string     Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with "docker
                           context use")
  -D, --debug              Enable debug mode
  -H, --host list          Daemon socket(s) to connect to
  -l, --log-level string   Set the logging level ("debug"|"info"|"warn"|"error"|"fatal") (default "info")
      --tls                Use TLS; implied by --tlsverify
      --tlscacert string   Trust certs signed only by this CA (default "/Users/lzw/.docker/ca.pem")
      --tlscert string     Path to TLS certificate file (default "/Users/lzw/.docker/cert.pem")
      --tlskey string      Path to TLS key file (default "/Users/lzw/.docker/key.pem")
      --tlsverify          Use TLS and verify the remote
  -v, --version            Print version information and quit

Management Commands:
  app*        Docker App (Docker Inc., v0.9.1-beta3)
  builder     Manage builds
  buildx*     Build with BuildKit (Docker Inc., v0.5.1-docker)
  config      Manage Docker configs
  container   Manage containers
  context     Manage contexts
  image       Manage images
  manifest    Manage Docker image manifests and manifest lists
  network     Manage networks
  node        Manage Swarm nodes
  plugin      Manage plugins
  scan*       Docker Scan (Docker Inc., v0.5.0)
  secret      Manage Docker secrets
  service     Manage services
  stack       Manage Docker stacks
  swarm       Manage Swarm
  system      Manage Docker
  trust       Manage trust on Docker images
  volume      Manage volumes

Commands:
  attach      Attach local standard input, output, and error streams to a running container
  build       Build an image from a Dockerfile
  commit      Create a new image from a container's changes
  cp          Copy files/folders between a container and the local filesystem
  create      Create a new container
  diff        Inspect changes to files or directories on a container's filesystem
  events      Get real time events from the server
  exec        Run a command in a running container
  export      Export a container's filesystem as a tar archive
  history     Show the history of an image
  images      List images
  import      Import the contents from a tarball to create a filesystem image
  info        Display system-wide information
  inspect     Return low-level information on Docker objects
  kill        Kill one or more running containers
  load        Load an image from a tar archive or STDIN
  login       Log in to a Docker registry
  logout      Log out from a Docker registry
  logs        Fetch the logs of a container
  pause       Pause all processes within one or more containers
  port        List port mappings or a specific mapping for the container
  ps          List containers
  pull        Pull an image or a repository from a registry
  push        Push an image or a repository to a registry
  rename      Rename a container
  restart     Restart one or more containers
  rm          Remove one or more containers
  rmi         Remove one or more images
  run         Run a command in a new container
  save        Save one or more images to a tar archive (streamed to STDOUT by default)
  search      Search the Docker Hub for images
  start       Start one or more stopped containers
  stats       Display a live stream of container(s) resource usage statistics
  stop        Stop one or more running containers
  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE
  top         Display the running processes of a container
  unpause     Unpause all processes within one or more containers
  update      Update configuration of one or more containers
  version     Show the Docker version information
  wait        Block until one or more containers stop, then print their exit codes

Run 'docker COMMAND --help' for more information on a command.

To get more help with docker, check out our guides at https://docs.docker.com/go/guides/
```



ç…§ç€æ•™ç¨‹è¯•è¯•ã€‚

```shell
$ docker run -d -p 80:80 docker/getting-started
Unable to find image 'docker/getting-started:latest' locally
latest: Pulling from docker/getting-started
aad63a933944: Pull complete
b14da7a62044: Pull complete
343784d40d66: Pull complete
6f617e610986: Pull complete
Digest: sha256:d2c4fb0641519ea208f20ab03dc40ec2a5a53fdfbccca90bef14f870158ed577
Status: Downloaded newer image for docker/getting-started:latest
815f13fc8f99f6185257980f74c349e182842ca572fe60ff62cbb15641199eaf
docker: Error response from daemon: Ports are not available: listen tcp 0.0.0.0:80: bind: address already in use.
```

æ”¹ä¸ªç«¯å£ã€‚

```shell
$ docker run -d -p 8080:80 docker/getting-started
45bb95fa1ae80adc05cc498a1f4f339c45c51f7a8ae1be17f5b704853a5513a5
```

![docker_run](./img/docker_run.png)

æ‰“å¼€æµè§ˆå™¨ï¼Œè¯´æ˜æˆ‘ä»¬æŠŠ`docker`è¿è¡Œèµ·æ¥äº†ã€‚

![browser](./img/browser.png)

åœæ‰å®¹å™¨ã€‚ç”¨ä¸Šåˆšåˆšè¿”å›çš„`ID`ã€‚

```shell
$ docker stop 45bb95fa1ae80adc05cc498a1f4f339c45c51f7a8ae1be17f5b704853a5513a5
45bb95fa1ae80adc05cc498a1f4f339c45c51f7a8ae1be17f5b704853a5513a5
```

è¿™æ—¶å·²ç»æ‰“ä¸å¼€ç½‘å€äº†ã€‚



è¿™è¯´æ˜`docker`åƒæ˜¯è™šæ‹Ÿæœºã€‚



## Flink



æ‰“å¼€å®˜ç½‘ã€‚

![flink-home-graphic](./img/flink-home-graphic.png)

`Flink`æ˜¯è¯´æ•°æ®æµçš„`Stateful`è®¡ç®—ã€‚`Stateful`æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿæš‚æ—¶è¿˜ä¸æ˜ç™½ã€‚ä¸Šé¢è¿™ä¸ªå›¾è¿˜æ˜¯å¾ˆæœ‰è¶£çš„ã€‚æ¥è¯•è¯•çœ‹ã€‚



è¯´æ˜¯éœ€è¦Javaç¯å¢ƒã€‚

```shell
$ java -version
java version "1.8.0_151"
Java(TM) SE Runtime Environment (build 1.8.0_151-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)
```

ä»å®˜ç½‘ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ `flink-1.12.2-bin-scala_2.11.tar`ã€‚



```shell
$ ./bin/start-cluster.sh
Starting cluster.
Starting standalonesession daemon on host lzwjava.
Starting taskexecutor daemon on host lzwjava.
```



```shell
$ ./bin/flink run examples/streaming/WordCount.jar
Executing WordCount example with default input data set.
Use --input to specify file input.
Printing result to stdout. Use --output to specify output path.
Job has been submitted with JobID 60f37647c20c2a6654359bd34edab807
Program execution finished
Job with JobID 60f37647c20c2a6654359bd34edab807 has finished.
Job Runtime: 757 ms
```



```shell
$ tail log/flink-*-taskexecutor-*.out
(nymph,1)
(in,3)
(thy,1)
(orisons,1)
(be,4)
(all,2)
(my,1)
(sins,1)
(remember,1)
(d,4)
```



```shell
$ ./bin/stop-cluster.sh
Stopping taskexecutor daemon (pid: 41812) on host lzwjava.
```



å—¯ï¼Œä¸Šæ‰‹æˆåŠŸã€‚å¯è§è¿™è·Ÿ`Spark`å¾ˆåƒã€‚



## Kylin



æ¥æ‰“å¼€å®˜ç½‘ã€‚



> Apache Kylinâ„¢ is an open source, distributed Analytical Data Warehouse for Big Data; it was designed to provide OLAP (Online Analytical Processing) capability in the big data era. By renovating the multi-dimensional cube and precalculation technology on Hadoop and Spark, Kylin is able to achieve near constant query speed regardless of the ever-growing data volume. Reducing query latency from minutes to sub-second, Kylin brings online analytics back to big data.



> Apache Kylinâ„¢ lets you query billions of rows at sub-second latency in 3 steps.
>
> 1. Identify a Star/Snowflake Schema on Hadoop.
> 2. Build Cube from the identified tables.
> 3. Query using ANSI-SQL and get results in sub-second, via ODBC, JDBC or RESTful API.

![kylin_diagram](/Users/lzw/curiosity-courses/distributed/img/kylin_diagram.png)



å¤§æ¦‚å°±æ˜¯åˆ†æå¤§æ•°æ®çš„ä¸€å±‚ã€‚ç”¨å®ƒå¯ä»¥æŸ¥å¾—éå¸¸å¿«ã€‚ä½œä¸ºæ¡¥æ¢ã€‚



å¯æƒœå½“å‰åªèƒ½åœ¨`Linux`ç¯å¢ƒä¸‹ä½¿ç”¨ã€‚å›å¤´å†æ¥æŠ˜è…¾ã€‚



## MongoDB



è¿™ä¹Ÿæ˜¯ä¸€ç§æ•°æ®åº“ã€‚è¯•è¯•å®‰è£…ã€‚



```shell
$ brew tap mongodb/brew
==> Tapping mongodb/brew
Cloning into '/usr/local/Homebrew/Library/Taps/mongodb/homebrew-brew'...
remote: Enumerating objects: 63, done.
remote: Counting objects: 100% (63/63), done.
remote: Compressing objects: 100% (62/62), done.
remote: Total 566 (delta 21), reused 6 (delta 1), pack-reused 503
Receiving objects: 100% (566/566), 121.78 KiB | 335.00 KiB/s, done.
Resolving deltas: 100% (259/259), done.
Tapped 11 formulae (39 files, 196.2KB).
```



```shell
$ brew install mongodb-community@4.4
==> Installing mongodb-community from mongodb/brew
==> Downloading https://fastdl.mongodb.org/tools/db/mongodb-database-tools-macos-x86_64-100.3.0.zip
######################################################################## 100.0%
==> Downloading https://fastdl.mongodb.org/osx/mongodb-macos-x86_64-4.4.3.tgz
######################################################################## 100.0%
==> Installing dependencies for mongodb/brew/mongodb-community: mongodb-database-tools
==> Installing mongodb/brew/mongodb-community dependency: mongodb-database-tools
Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink bin/bsondump
Target /usr/local/bin/bsondump
is a symlink belonging to mongodb. You can unlink it:
  brew unlink mongodb

To force the link and overwrite all conflicting files:
  brew link --overwrite mongodb-database-tools

To list all files that would be deleted:
  brew link --overwrite --dry-run mongodb-database-tools

Possible conflicting files are:
/usr/local/bin/bsondump -> /usr/local/Cellar/mongodb/3.0.7/bin/bsondump
/usr/local/bin/mongodump -> /usr/local/Cellar/mongodb/3.0.7/bin/mongodump
/usr/local/bin/mongoexport -> /usr/local/Cellar/mongodb/3.0.7/bin/mongoexport
/usr/local/bin/mongofiles -> /usr/local/Cellar/mongodb/3.0.7/bin/mongofiles
/usr/local/bin/mongoimport -> /usr/local/Cellar/mongodb/3.0.7/bin/mongoimport
/usr/local/bin/mongorestore -> /usr/local/Cellar/mongodb/3.0.7/bin/mongorestore
/usr/local/bin/mongostat -> /usr/local/Cellar/mongodb/3.0.7/bin/mongostat
/usr/local/bin/mongotop -> /usr/local/Cellar/mongodb/3.0.7/bin/mongotop
==> Summary
ğŸº  /usr/local/Cellar/mongodb-database-tools/100.3.0: 13 files, 154MB, built in 11 seconds
==> Installing mongodb/brew/mongodb-community
Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink bin/mongo
Target /usr/local/bin/mongo
is a symlink belonging to mongodb. You can unlink it:
  brew unlink mongodb

To force the link and overwrite all conflicting files:
  brew link --overwrite mongodb-community

To list all files that would be deleted:
  brew link --overwrite --dry-run mongodb-community

Possible conflicting files are:
/usr/local/bin/mongo -> /usr/local/Cellar/mongodb/3.0.7/bin/mongo
/usr/local/bin/mongod -> /usr/local/Cellar/mongodb/3.0.7/bin/mongod
/usr/local/bin/mongos -> /usr/local/Cellar/mongodb/3.0.7/bin/mongos
==> Caveats
To have launchd start mongodb/brew/mongodb-community now and restart at login:
  brew services start mongodb/brew/mongodb-community
Or, if you don't want/need a background service you can just run:
  mongod --config /usr/local/etc/mongod.conf
==> Summary
ğŸº  /usr/local/Cellar/mongodb-community/4.4.3: 11 files, 156.8MB, built in 10 seconds
==> Caveats
==> mongodb-community
To have launchd start mongodb/brew/mongodb-community now and restart at login:
  brew services start mongodb/brew/mongodb-community
Or, if you don't want/need a background service you can just run:
  mongod --config /usr/local/etc/mongod.conf
```



ä¹‹å‰æˆ‘å®‰è£…ä¸€ä¸ªæ—§ç‰ˆæœ¬çš„ã€‚è§£é™¤ä¸€ä¸‹é“¾æ¥ã€‚

```shell
$ brew unlink mongodb
Unlinking /usr/local/Cellar/mongodb/3.0.7... 11 symlinks removed
```

```shell
$ mongod --version
db version v4.4.3
Build Info: {
    "version": "4.4.3",
    "gitVersion": "913d6b62acfbb344dde1b116f4161360acd8fd13",
    "modules": [],
    "allocator": "system",
    "environment": {
        "distarch": "x86_64",
        "target_arch": "x86_64"
    }
}
```



æ¥ç€è¿è¡Œ`mongod`å¯åŠ¨mongoæ•°æ®åº“æœåŠ¡å™¨ã€‚ç„¶è€Œç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶è¯´`/data/db`ä¸å­˜åœ¨ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œ`~/mongodb` ï¼Œè¿™é‡Œæ¥ä¿å­˜æ•°æ®åº“æ–‡ä»¶ã€‚

```shell
$ mongod --dbpath ~/mongodb
```



è¾“å‡ºä¸ºï¼š

```json
{"t":{"$date":"2021-03-11T18:17:32.838+08:00"},"s":"I",  "c":"CONTROL",  "id":23285,   "ctx":"main","msg":"Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'"}
{"t":{"$date":"2021-03-11T18:17:32.842+08:00"},"s":"W",  "c":"ASIO",     "id":22601,   "ctx":"main","msg":"No TransportLayer configured during NetworkInterface startup"}
{"t":{"$date":"2021-03-11T18:17:32.842+08:00"},"s":"I",  "c":"NETWORK",  "id":4648602, "ctx":"main","msg":"Implicit TCP FastOpen in use."}
{"t":{"$date":"2021-03-11T18:17:32.842+08:00"},"s":"I",  "c":"STORAGE",  "id":4615611, "ctx":"initandlisten","msg":"MongoDB starting","attr":{"pid":46256,"port":27017,"dbPath":"/Users/lzw/mongodb","architecture":"64-bit","host":"lzwjava"}}
{"t":{"$date":"2021-03-11T18:17:32.842+08:00"},"s":"I",  "c":"CONTROL",  "id":23403,   "ctx":"initandlisten","msg":"Build Info","attr":{"buildInfo":{"version":"4.4.3","gitVersion":"913d6b62acfbb344dde1b116f4161360acd8fd13","modules":[],"allocator":"system","environment":{"distarch":"x86_64","target_arch":"x86_64"}}}}
{"t":{"$date":"2021-03-11T18:17:32.843+08:00"},"s":"I",  "c":"CONTROL",  "id":51765,   "ctx":"initandlisten","msg":"Operating System","attr":{"os":{"name":"Mac OS X","version":"20.3.0"}}}
...
```

å¯è§éƒ½æ˜¯`JSON`æ ¼å¼ã€‚MongoDBå°±æ˜¯ä¸€åˆ‡æ•°æ®æ–‡ä»¶éƒ½æ˜¯ç”¨`JSON`æ ¼å¼æ¥ä¿å­˜çš„ã€‚æ¥ç€ï¼Œæ‰“å¼€å¦å¤–ä¸€ä¸ªç»ˆç«¯æ ‡ç­¾ã€‚

```shell
$ mongo
MongoDB shell version v4.4.3
connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("4f55c561-70d3-4289-938d-4b90a284891f") }
MongoDB server version: 4.4.3
---
The server generated these startup warnings when booting:
        2021-03-11T18:17:33.743+08:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted
        2021-03-11T18:17:33.743+08:00: This server is bound to localhost. Remote systems will be unable to connect to this server. Start the server with --bind_ip <address> to specify which IP addresses it should serve responses from, or with --bind_ip_all to bind to all interfaces. If this behavior is desired, start the server with --bind_ip 127.0.0.1 to disable this warning
        2021-03-11T18:17:33.743+08:00: Soft rlimits too low
        2021-03-11T18:17:33.743+08:00:         currentValue: 4864
        2021-03-11T18:17:33.743+08:00:         recommendedMinimum: 64000
---
---
        Enable MongoDB's free cloud-based monitoring service, which will then receive and display
        metrics about your deployment (disk utilization, CPU, operation statistics, etc).

        The monitoring data will be available on a MongoDB website with a unique URL accessible to you
        and anyone you share the URL with. MongoDB may use this information to make product
        improvements and to suggest MongoDB products and deployment options to you.

        To enable free monitoring, run the following command: db.enableFreeMonitoring()
        To permanently disable this reminder, run the following command: db.disableFreeMonitoring()
```



æ¥ç€å¯ä»¥å°è¯•æ’å…¥æ•°æ®ã€æŸ¥è¯¢æ•°æ®ã€‚

```shell
> db.inventory.insertOne(
...    { item: "canvas", qty: 100, tags: ["cotton"], size: { h: 28, w: 35.5, uom: "cm" } }
... )
{
	"acknowledged" : true,
	"insertedId" : ObjectId("6049ef91b653541cf355facb")
}
>
> db.inventory.find()
{ "_id" : ObjectId("6049ef91b653541cf355facb"), "item" : "canvas", "qty" : 100, "tags" : [ "cotton" ], "size" : { "h" : 28, "w" : 35.5, "uom" : "cm" } }
```



## æœ€å



å…ˆåˆ°è¿™å„¿ã€‚åé¢æˆ‘ä»¬å†ä¸Šæ‰‹åˆ«çš„å·¥å…·ã€‚æˆ‘ä»¬åšè¿™äº›æ„ä¹‰æ˜¯ä»€ä¹ˆã€‚å¤§æ¦‚æ˜¯å…ˆæœ‰ä¸ªè„‰ç»œã€‚ä¸‡äº‹å¼€å¤´éš¾ï¼Œè€Œæˆ‘ä»¬ä¸€ä¸Šæ¥å°±æŠŠè¿™äº›å…¨éƒ¨æŠ˜è…¾ä¸€éã€‚è¿™ç»™äº†æˆ‘ä»¬ä¿¡å¿ƒï¼Œæ¥ä¸‹æ¥ï¼Œå°±æ˜¯æ›´å¤šæŠ˜è…¾è¿™äº›è½¯ä»¶äº†ã€‚



## ç»ƒä¹ 

* å­¦ç”Ÿåƒä¸Šé¢ä¸€æ ·ç±»ä¼¼æ¢ç´¢ä¸€éã€‚







